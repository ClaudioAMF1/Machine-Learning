# -*- coding: utf-8 -*-
"""bank_Claudio_Meireles.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JtJUvP72prJbDh1GCcu8TjC1nnXERI0m
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import warnings
warnings.filterwarnings('ignore')

def load_and_explore_data():
    """Carrega e realiza análise exploratória inicial dos dados"""
    print("1. Carregamento e Análise Exploratória dos Dados")

    url = 'https://raw.githubusercontent.com/bluenex/WekaLearningDataset/refs/heads/master/bank/bank-data.csv'
    df = pd.read_csv(url)
    print("\nDimensões do dataset:", df.shape)

    print("\nInformações do dataset:")
    print(df.info())

    print("\nEstatísticas descritivas:")
    print(df.describe())

    plt.figure(figsize=(10, 5))
    sns.countplot(data=df, x='pep')
    plt.title('Distribuição de PEP')
    plt.show()

    numeric_cols = ['age', 'income', 'children']
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    for i, col in enumerate(numeric_cols):
        sns.boxplot(data=df, x='pep', y=col, ax=axes[i])
        axes[i].set_title(f'{col} por PEP')

    plt.tight_layout()
    plt.show()

    # Análise de variáveis categóricas
    categorical_cols = ['sex', 'region', 'married', 'car', 'save_act', 'current_act', 'mortgage']
    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    axes = axes.ravel()

    for i, col in enumerate(categorical_cols):
        sns.countplot(data=df, x=col, hue='pep', ax=axes[i])
        axes[i].set_title(f'PEP por {col}')
        axes[i].tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

    return df

# 2. Pré-processamento
def preprocess_data(df):
    """Realiza o pré-processamento dos dados"""
    print("\n2. Pré-processamento dos Dados")

    # Remover coluna ID
    df = df.drop('id', axis=1)

    # Encoding de variáveis categóricas
    categorical_cols = ['sex', 'region', 'married', 'car', 'save_act', 'current_act', 'mortgage', 'pep']
    le = LabelEncoder()

    for col in categorical_cols:
        df[col] = le.fit_transform(df[col])

    # Separar features e target
    X = df.drop('pep', axis=1)
    y = df['pep']

    # Split dos dados
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Normalização
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    return X_train_scaled, X_test_scaled, y_train, y_test, X.columns

# 3. Modelagem e Avaliação
def train_and_evaluate_models(X_train, X_test, y_train, y_test):
    """Treina e avalia diferentes modelos"""
    print("\n3. Treinamento e Avaliação dos Modelos")

    # Definição dos modelos
    models = {
        'Logistic Regression': LogisticRegression(),
        'Decision Tree': DecisionTreeClassifier(random_state=42),
        'KNN': KNeighborsClassifier(),
        'SVM': SVC(probability=True, random_state=42),
        'Random Forest': RandomForestClassifier(random_state=42)
    }

    results = {}
    for name, model in models.items():
        print(f"\nTreinando {name}...")

        # Cross-validation
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')

        # Treino e avaliação
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # Métricas
        print(f"\nResultados para {name}:")
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred))

        # ROC Curve
        if hasattr(model, "predict_proba"):
            y_pred_proba = model.predict_proba(X_test)[:, 1]
            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
            roc_auc = auc(fpr, tpr)

            plt.figure(figsize=(8, 6))
            plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve - {name}')
            plt.legend(loc='lower right')
            plt.show()

        results[name] = {
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'model': model
        }

    return results

# 4. Otimização de Hiperparâmetros
def optimize_best_model(best_model_name, X_train, y_train, X_test, y_test):
    """Otimiza os hiperparâmetros do melhor modelo"""
    print("\n4. Otimização de Hiperparâmetros")

    if best_model_name == 'Random Forest':
        param_dist = {
            'n_estimators': [100, 200, 300],
            'max_depth': [None, 10, 20, 30],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        model = RandomForestClassifier(random_state=42)

    elif best_model_name == 'SVM':
        param_dist = {
            'C': [0.1, 1, 10],
            'kernel': ['rbf', 'linear'],
            'gamma': ['scale', 'auto']
        }
        model = SVC(probability=True, random_state=42)

    else:
        return None, None

    # Randomized Search
    random_search = RandomizedSearchCV(
        model, param_distributions=param_dist,
        n_iter=10, cv=5, scoring='f1',
        random_state=42, n_jobs=-1
    )

    random_search.fit(X_train, y_train)
    print("\nMelhores parâmetros:", random_search.best_params_)

    # Avaliação final
    best_model = random_search.best_estimator_
    y_pred = best_model.predict(X_test)
    print("\nResultados do modelo otimizado:")
    print(classification_report(y_test, y_pred))

    return best_model, random_search.best_params_

# 5. Análise de Importância das Features
def analyze_feature_importance(model, feature_names):
    """Analisa a importância das features"""
    print("\n5. Análise de Importância das Features")

    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1]

        plt.figure(figsize=(10, 6))
        plt.title('Importância das Features')
        plt.bar(range(len(importances)), importances[indices])
        plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45)
        plt.tight_layout()
        plt.show()

        for i in indices:
            print(f"{feature_names[i]}: {importances[i]:.4f}")

def main():
    # 1. Carregamento e EDA
    df = load_and_explore_data()

    # 2. Pré-processamento
    X_train, X_test, y_train, y_test, feature_names = preprocess_data(df)

    # 3. Modelagem inicial
    results = train_and_evaluate_models(X_train, X_test, y_train, y_test)

    # 4. Otimização do melhor modelo
    best_model_name = max(results.items(), key=lambda x: x[1]['cv_mean'])[0]
    print(f"\nMelhor modelo: {best_model_name}")

    best_model, best_params = optimize_best_model(
        best_model_name,
        X_train, y_train,
        X_test, y_test
    )

    # 5. Análise final
    if best_model is not None:
        analyze_feature_importance(best_model, feature_names)

if __name__ == "__main__":
    main()