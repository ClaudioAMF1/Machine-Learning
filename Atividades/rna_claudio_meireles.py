# -*- coding: utf-8 -*-
"""RNA_Claudio_Meireles.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qSbJg32S_pAkN-U3n5EKWOdUBGt-E7di
"""

import pandas as pd
import numpy as np
import requests
import zipfile
import io
import calendar
from datetime import datetime
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import seaborn as sns

def load_data():
    """Carrega o dataset"""
    print("1. Carregamento do Dataset")
    url = "https://raw.githubusercontent.com/klaytoncastro/idp-machinelearning/main/resources/online_retail.zip"
    response = requests.get(url)
    with zipfile.ZipFile(io.BytesIO(response.content)) as z:
        with z.open('online_retail_dataset.csv') as f:
            df = pd.read_csv(f)
    return df

def preprocess_data(df):
    """Pré-processamento inicial dos dados"""
    print("\n2. Pré-processamento dos dados")
    df = df.copy()

    # Convertendo data e tratando tipos
    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
    df['TotalAmount'] = df['Quantity'] * df['UnitPrice']

    # Removendo valores inválidos
    df = df.dropna()
    df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]
    df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]

    return df

def analyze_time_patterns(df):
    """Análise de padrões temporais"""
    print("\n3. Analisando padrões temporais")

    # Análise mensal
    monthly_data = df.set_index('InvoiceDate').resample('M')['TotalAmount'].sum().reset_index()
    monthly_data['MonthYear'] = monthly_data['InvoiceDate'].dt.strftime('%B-%Y')

    # Análise diária
    daily_data = df.groupby(df['InvoiceDate'].dt.dayofweek)['TotalAmount'].mean()

    # **Create a full index with all days of the week**
    all_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    daily_data = daily_data.reindex(range(7)).fillna(0) # Fill missing days with 0
    daily_data.index = all_days  # Now assign the full index

    # Análise horária
    hourly_data = df.groupby(df['InvoiceDate'].dt.hour)['TotalAmount'].mean()

    return monthly_data, daily_data, hourly_data

def plot_temporal_patterns(monthly_data, daily_data, hourly_data):
    """Visualização dos padrões temporais"""
    plt.figure(figsize=(15, 5))

    # Tendência mensal
    plt.subplot(131)
    plt.plot(range(len(monthly_data)), monthly_data['TotalAmount'])
    plt.title('Tendência Mensal de Vendas')
    plt.xticks(range(len(monthly_data)), monthly_data['MonthYear'], rotation=45)

    # Padrão diário
    plt.subplot(132)
    daily_data.plot(kind='bar')
    plt.title('Média de Vendas por Dia')

    # Padrão horário
    plt.subplot(133)
    hourly_data.plot(kind='bar')
    plt.title('Média de Vendas por Hora')

    plt.tight_layout()
    plt.show()

def perform_rfm_analysis(df):
    """Análise RFM"""
    print("\n4. Realizando análise RFM")

    # Calcular métricas RFM
    today = df['InvoiceDate'].max()
    rfm = df.groupby('CustomerID').agg({
        'InvoiceDate': lambda x: (today - x.max()).days,  # Recency
        'InvoiceNo': 'count',  # Frequency
        'TotalAmount': 'sum'  # Monetary
    })

    rfm.columns = ['Recency', 'Frequency', 'Monetary']

    # Scores
    rfm['R_Score'] = pd.qcut(rfm['Recency'], q=5, labels=[5,4,3,2,1])
    rfm['F_Score'] = pd.qcut(rfm['Frequency'], q=5, labels=[1,2,3,4,5])
    rfm['M_Score'] = pd.qcut(rfm['Monetary'], q=5, labels=[1,2,3,4,5])

    return rfm

class EnhancedAutoencoder:
    def __init__(self, input_dim, encoding_dim=32):
        self.input_dim = input_dim
        self.encoding_dim = encoding_dim
        self.autoencoder, self.encoder = self._build_model()

    def _build_model(self):
        # Encoder
        input_layer = Input(shape=(self.input_dim,))
        encoded = Dense(self.encoding_dim * 2, activation='relu')(input_layer)
        encoded = Dense(self.encoding_dim, activation='relu')(encoded)

        # Decoder
        decoded = Dense(self.encoding_dim * 2, activation='relu')(encoded)
        decoded = Dense(self.input_dim, activation='sigmoid')(decoded)

        # Models
        autoencoder = Model(input_layer, decoded)
        encoder = Model(input_layer, encoded)

        autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

        return autoencoder, encoder

    def train(self, X_train, epochs=50, batch_size=32):
        return self.autoencoder.fit(
            X_train, X_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=0.2,
            callbacks=[EarlyStopping(patience=5)],
            verbose=1
        )

class EnhancedMLP:
    def __init__(self, input_dim):
        self.model = self._build_model(input_dim)

    def _build_model(self, input_dim):
        model = Sequential([
            Dense(64, activation='relu'),
            Dropout(0.3),
            Dense(32, activation='relu'),
            Dropout(0.2),
            Dense(1, activation='sigmoid')
        ])

        model.compile(optimizer=Adam(learning_rate=0.001),
                     loss='binary_crossentropy',
                     metrics=['accuracy'])
        return model

    def train(self, X_train, y_train, epochs=50, batch_size=32):
        return self.model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=0.2,
            callbacks=[EarlyStopping(patience=5)],
            verbose=1
        )

    def predict(self, X):
        return self.model.predict(X)

def prepare_data_for_models(df):
    """Prepara dados para os modelos"""
    # Matriz de produtos por cliente
    product_matrix = pd.pivot_table(
        df,
        values='Quantity',
        index='CustomerID',
        columns='StockCode',
        aggfunc='sum',
        fill_value=0
    )

    # Features para MLP
    customer_features = df.groupby('CustomerID').agg({
        'Quantity': ['sum', 'mean'],
        'UnitPrice': 'mean',
        'TotalAmount': 'sum'
    }).fillna(0)

    customer_features.columns = ['_'.join(col) if isinstance(col, tuple) else col
                               for col in customer_features.columns]

    return product_matrix, customer_features

def analyze_results(autoencoder, mlp, X_test_ae, X_test_mlp, y_test):
    """Análise dos resultados dos modelos"""
    print("\n5. Análise dos resultados")

    # Autoencoder reconstruction
    decoded = autoencoder.autoencoder.predict(X_test_ae)  # Use X_test_ae here
    reconstruction_error = np.mean((X_test_ae - decoded) ** 2)  # Use X_test_ae here
    print(f"\nAutoencoder reconstruction error: {reconstruction_error:.4f}")

    # MLP predictions
    y_pred = mlp.predict(X_test_mlp)  # Changed to X_test_mlp
    print("\nClassification Report:")
    print(classification_report(y_test, (y_pred > 0.5).astype(int)))

    return reconstruction_error, y_pred

def main():
    # 1. Carregamento e pré-processamento
    df = load_data()
    df_processed = preprocess_data(df)

    # 2. Análise temporal
    monthly_data, daily_data, hourly_data = analyze_time_patterns(df_processed)
    plot_temporal_patterns(monthly_data, daily_data, hourly_data)

    # 3. Análise RFM
    rfm = perform_rfm_analysis(df_processed)

    # 4. Preparação dos dados para os modelos
    product_matrix, customer_features = prepare_data_for_models(df_processed)

    # 5. Normalização
    scaler = StandardScaler()
    X_product = scaler.fit_transform(product_matrix)
    X_customer = scaler.fit_transform(customer_features)

    # 6. Split dos dados
    X_train_ae, X_test_ae = train_test_split(X_product, test_size=0.2, random_state=42)

    # Target para MLP (alto valor vs baixo valor)
    y = (customer_features['TotalAmount_sum'] >
         customer_features['TotalAmount_sum'].median()).astype(int)
    X_train_mlp, X_test_mlp, y_train_mlp, y_test_mlp = train_test_split(
        X_customer, y, test_size=0.2, random_state=42
    )

    # 7. Treinamento dos modelos
    print("\nTreinando modelos...")
    autoencoder = EnhancedAutoencoder(input_dim=X_product.shape[1])
    autoencoder.train(X_train_ae)

    mlp = EnhancedMLP(input_dim=X_customer.shape[1])
    mlp.train(X_train_mlp, y_train_mlp)

    # 8. Análise dos resultados
    reconstruction_error, y_pred = analyze_results(
        autoencoder, mlp, X_test_ae, X_test_mlp, y_test_mlp
    )

if __name__ == "__main__":
    main()

def analyze_results(autoencoder, mlp, X_test_ae, X_test_mlp, y_test):
    """Análise dos resultados dos modelos"""
    print("\n5. Análise dos resultados")

    # Autoencoder reconstruction
    decoded = autoencoder.autoencoder.predict(X_test_ae)  # Use X_test_ae here
    reconstruction_error = np.mean((X_test_ae - decoded) ** 2)  # Use X_test_ae here
    print(f"\nAutoencoder reconstruction error: {reconstruction_error:.4f}")

    # MLP predictions
    y_pred = mlp.predict(X_test_mlp)  # Changed to X_test_mlp
    print("\nClassification Report:")
    print(classification_report(y_test, (y_pred > 0.5).astype(int)))

    return reconstruction_error, y_pred

def main():
    # ... (other parts of the code remain the same) ...

    # 8. Análise dos resultados
    reconstruction_error, y_pred = analyze_results(
        autoencoder, mlp, X_test_ae, X_test_mlp, y_test_mlp  # Added X_test_mlp
    )