# -*- coding: utf-8 -*-
"""airquality_ClaudioMeireles.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jp5uZ2mGyTQrlio3xQXMku6jqaQp6c04
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

def load_and_prepare_data():
    """Carrega e prepara os dados iniciais"""
    print("1. Carregando dados...")
    url = 'https://raw.githubusercontent.com/klaytoncastro/idp-storytelling/master/airquality/airquality.csv'
    df = pd.read_csv(url, delimiter=';', decimal=',')

    df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%d/%m/%Y %H.%M.%S')
    df.drop(columns=['Date', 'Time'], inplace=True)

    df.replace(-200, np.nan, inplace=True)
    df.drop(columns=['NMHC(GT)'], inplace=True)

    df = df.fillna(method='ffill').fillna(method='bfill')

    return df

def perform_eda(df):
    """Análise exploratória completa"""
    print("\n2. Análise exploratória...")

    pollutants = ['CO(GT)', 'NO2(GT)', 'NOx(GT)', 'C6H6(GT)']
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    for i, pol in enumerate(pollutants):
        ax = axes[i//2, i%2]
        sns.boxplot(data=df, y=pol, ax=ax)
        ax.set_title(f'Distribuição de {pol}')
    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(12, 8))
    sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
    plt.title('Matriz de Correlação')
    plt.show()

    df.set_index('DateTime', inplace=True)
    fig, axes = plt.subplots(2, 1, figsize=(15, 10))

    daily_mean = df['CO(GT)'].resample('D').mean()
    axes[0].plot(daily_mean.index, daily_mean.values)
    axes[0].set_title('Média Diária de CO')

    hourly_mean = df.groupby(df.index.hour)['CO(GT)'].mean()
    axes[1].plot(hourly_mean.index, hourly_mean.values)
    axes[1].set_title('Padrão Diário de CO')
    plt.tight_layout()
    plt.show()

    return df

def prepare_features(df):
    """Preparação completa das features"""
    print("\n3. Preparando features...")

    df['Hour'] = df.index.hour
    df['Month'] = df.index.month
    df['DayOfWeek'] = df.index.dayofweek

    df['Hour_sin'] = np.sin(2 * np.pi * df['Hour']/24)
    df['Hour_cos'] = np.cos(2 * np.pi * df['Hour']/24)
    df['Month_sin'] = np.sin(2 * np.pi * df['Month']/12)
    df['Month_cos'] = np.cos(2 * np.pi * df['Month']/12)

    features = ['PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)',
                'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)',
                'T', 'RH', 'AH', 'Hour_sin', 'Hour_cos', 'Month_sin', 'Month_cos',
                'DayOfWeek']

    X = df[features]
    y = df['CO(GT)']

    return X, y, features

def train_and_evaluate_models(X, y):
    """Treinamento e avaliação de todos os modelos solicitados"""
    print("\n4. Treinando e avaliando modelos...")

    # Split e normalização
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Definição dos modelos
    models = {
        'Linear Regression': LinearRegression(),
        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
        'Extra Trees': ExtraTreesRegressor(n_estimators=100, random_state=42),
        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
        'KNN': KNeighborsRegressor(n_neighbors=5),
        'SVR': SVR(kernel='rbf')
    }

    results = {}
    for name, model in models.items():
        print(f"\nAvaliando {name}...")

        # Cross-validation
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')

        # Treino e avaliação
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)

        results[name] = {
            'CV_R2': cv_scores.mean(),
            'CV_std': cv_scores.std(),
            'Test_R2': r2_score(y_test, y_pred),
            'Test_RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
            'Test_MAE': mean_absolute_error(y_test, y_pred)
        }

        print(f"R² CV: {results[name]['CV_R2']:.4f} (+/- {results[name]['CV_std']*2:.4f})")
        print(f"R² Test: {results[name]['Test_R2']:.4f}")
        print(f"RMSE: {results[name]['Test_RMSE']:.4f}")

    return results, models, (X_train_scaled, X_test_scaled, y_train, y_test)

def optimize_best_model(model_name, X_train, y_train, X_test, y_test):
    """Otimização rápida do melhor modelo"""
    print("\n5. Otimizando modelo...")

    if model_name in ['Random Forest', 'Extra Trees']:
        param_dist = {
            'n_estimators': [100, 200],
            'max_depth': [None, 10, 20],
            'min_samples_split': [2, 5],
            'min_samples_leaf': [1, 2]
        }

        base_model = RandomForestRegressor(random_state=42) if model_name == 'Random Forest' else ExtraTreesRegressor(random_state=42)

        # RandomizedSearchCV com poucas iterações para manter performance
        random_search = RandomizedSearchCV(
            base_model, param_distributions=param_dist,
            n_iter=5, cv=3, random_state=42, n_jobs=-1
        )

        random_search.fit(X_train, y_train)
        best_model = random_search.best_estimator_

        # Avaliação final
        y_pred = best_model.predict(X_test)
        print("\nResultados finais:")
        print(f"R² Score: {r2_score(y_test, y_pred):.4f}")
        print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")

        return best_model, random_search.best_params_

    return None, None

def analyze_model_and_features(model, X_test, y_test, feature_names):
    """Análise detalhada do modelo e features"""
    print("\n6. Analisando modelo e features...")

    y_pred = model.predict(X_test)

    # 1. Resíduos
    residuals = y_test - y_pred
    plt.figure(figsize=(15, 5))

    plt.subplot(131)
    plt.scatter(y_pred, residuals, alpha=0.5)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('Valores Previstos')
    plt.ylabel('Resíduos')
    plt.title('Análise de Resíduos')

    # 2. Importância das features
    if hasattr(model, 'feature_importances_'):
        plt.subplot(132)
        importances = pd.Series(model.feature_importances_, index=feature_names)
        importances.sort_values().plot(kind='barh')
        plt.title('Importância das Features')

    # 3. Previsões vs Reais
    plt.subplot(133)
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('Valores Reais')
    plt.ylabel('Valores Previstos')
    plt.title('Previsões vs Valores Reais')

    plt.tight_layout()
    plt.show()

def main():
    # 1. Carregamento e preparação
    df = load_and_prepare_data()

    # 2. Análise exploratória
    df = perform_eda(df)

    # 3. Preparação das features
    X, y, feature_names = prepare_features(df)

    # 4. Treinamento inicial
    results, models, (X_train, X_test, y_train, y_test) = train_and_evaluate_models(X, y)

    # 5. Otimização do melhor modelo
    best_model_name = max(results.items(), key=lambda x: x[1]['Test_R2'])[0]
    print(f"\nMelhor modelo: {best_model_name}")

    best_model, best_params = optimize_best_model(
        best_model_name,
        X_train, y_train,
        X_test, y_test
    )

    # 6. Análise final
    if best_model is not None:
        analyze_model_and_features(best_model, X_test, y_test, feature_names)

if __name__ == "__main__":
    main()